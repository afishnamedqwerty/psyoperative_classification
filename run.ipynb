{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup and Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import GCNConv, GATConv, global_mean_pool\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Placeholder functions for loading JSON data files\n",
    "def load_json(file_name):\n",
    "    with open(file_name, 'r', encoding='utf-8') as file:\n",
    "        return json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Load user data (for illustration, assuming user_data is a list of dictionaries)\n",
    "user_data = load_json('user.json')  # Load user data\n",
    "\n",
    "# Define feature extraction function\n",
    "def extract_user_features(users):\n",
    "    features = []\n",
    "    for user in users:\n",
    "        # Behavioral features\n",
    "        tweet_frequency = user['tweet_count'] / ((datetime.now() - datetime.strptime(user['created_at'], '%a %b %d %H:%M:%S +0000 %Y')).days)\n",
    "        retweet_ratio = user['retweet_count'] / user['tweet_count'] if user['tweet_count'] else 0\n",
    "        mention_ratio = user['mention_count'] / user['tweet_count'] if user['tweet_count'] else 0\n",
    "        \n",
    "        # Content features\n",
    "        hashtag_ratio = user['hashtag_count'] / user['tweet_count'] if user['tweet_count'] else 0\n",
    "        url_ratio = user['url_count'] / user['tweet_count'] if user['tweet_count'] else 0\n",
    "        sensitive_content_ratio = user['sensitive_tweet_count'] / user['tweet_count'] if user['tweet_count'] else 0\n",
    "        \n",
    "        # Account features\n",
    "        account_age_days = (datetime.now() - datetime.strptime(user['created_at'], '%a %b %d %H:%M:%S +0000 %Y')).days\n",
    "        is_verified = int(user['verified'])\n",
    "        profile_completeness = int(user['description'] != '') + int(user['location'] != '') + int(user['profile_image_url'] != '')\n",
    "        \n",
    "        # Network features\n",
    "        followers_count = user['followers_count']\n",
    "        following_count = user['following_count']\n",
    "        follower_following_ratio = followers_count / following_count if following_count else followers_count\n",
    "        \n",
    "        features.append([\n",
    "            tweet_frequency, retweet_ratio, mention_ratio, hashtag_ratio, url_ratio, sensitive_content_ratio,\n",
    "            account_age_days, is_verified, profile_completeness, follower_following_ratio\n",
    "        ])\n",
    "    return features\n",
    "\n",
    "# Example feature extraction for users, extendable to other entities\n",
    "def extract_features(users):\n",
    "    features = np.array([\n",
    "        [user['followers_count'], user['following_count']]  # Continue with other features\n",
    "        for user in users\n",
    "    ])\n",
    "    return StandardScaler().fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load entity data\n",
    "users_data = load_json('user.json')\n",
    "tweets_data = load_json('tweet_0.json')  # Do this for tweet_[0-8].json\n",
    "lists_data = load_json('list.json')\n",
    "hashtags_data = load_json('hashtag.json')\n",
    "\n",
    "# Load relationships\n",
    "nodes = pd.read_csv('node.json')\n",
    "edges = pd.read_csv('edge.csv')\n",
    "\n",
    "# Load labels and splits\n",
    "labels = pd.read_csv('label.csv')\n",
    "splits = pd.read_csv('split.csv')\n",
    "\n",
    "user_features = extract_features(users_data)  # Do this for tweets, lists, and hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features\n",
    "#user_features = extract_user_features(user_data)\n",
    "\n",
    "# Convert to pandas DataFrame (for subsequent normalization and training)\n",
    "features_df = pd.DataFrame(user_features, columns=[\n",
    "    'tweet_frequency', 'retweet_ratio', 'mention_ratio', 'hashtag_ratio', 'url_ratio', \n",
    "    'sensitive_content_ratio', 'account_age_days', 'is_verified', 'profile_completeness', \n",
    "    'follower_following_ratio'\n",
    "])\n",
    "\n",
    "# Normalize and scale features\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(features_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_subgraph_sampling(graph, root_node, walk_length=30, num_walks=20):\n",
    "    from random import choice\n",
    "    \n",
    "    subgraphs = []\n",
    "    for _ in range(num_walks):\n",
    "        subgraph_nodes = set([root_node])\n",
    "        current_node = root_node\n",
    "        for _ in range(walk_length):\n",
    "            neighbors = list(graph.neighbors(current_node))\n",
    "            if neighbors:\n",
    "                next_node = choice(neighbors)\n",
    "                subgraph_nodes.add(next_node)\n",
    "                current_node = next_node\n",
    "            else:\n",
    "                break\n",
    "        induced_subgraph = graph.subgraph(subgraph_nodes).copy()\n",
    "        subgraphs.append(induced_subgraph)\n",
    "    return subgraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.from_pandas_edgelist(edges, source='source_id', target='target_id', create_using=nx.DiGraph())\n",
    "\n",
    "# Ensure this function is robust and handles the variety of entity and relation types\n",
    "dynamic_subgraphs = dynamic_subgraph_sampling(G, start_node='some_user_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_graph_data(edges, node_features):\n",
    "    node_encoder = LabelEncoder()\n",
    "    edges['source_encoded'] = node_encoder.fit_transform(edges['source_id'])\n",
    "    edges['target_encoded'] = node_encoder.transform(edges['target_id'])\n",
    "    \n",
    "    edge_index = torch.tensor([edges['source_encoded'].values, edges['target_encoded'].values], dtype=torch.long)\n",
    "    \n",
    "    node_features['node_id_encoded'] = node_encoder.transform(node_features['node_id'])\n",
    "    node_features = node_features.sort_values('node_id_encoded')\n",
    "    node_features_tensor = torch.tensor(node_features['features'].tolist(), dtype=torch.float)\n",
    "    \n",
    "    return edge_index.to(device), node_features_tensor.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#edges = pd.read_csv('edges.csv')  # Adjust with actual path and columns\n",
    "#node_features = pd.read_csv('node_features.csv')  # Adjust with actual path and columns\n",
    "edge_index, node_features = prepare_graph_data(edges, nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedGCN(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(EnhancedGCN, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, 2 * out_channels)\n",
    "        self.conv2 = GATConv(2 * out_channels, out_channels, heads=3)  # Using GAT for the second layer\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        return x\n",
    "\n",
    "class GraphBotDetector(nn.Module):\n",
    "    def __init__(self, feature_dim, num_classes):\n",
    "        super(GraphBotDetector, self).__init__()\n",
    "        self.feature_extractor = EnhancedGCN(feature_dim, 128)\n",
    "        self.pool = global_mean_pool\n",
    "        self.fc = nn.Linear(128 * 3, num_classes)  # Adjust according to GAT heads\n",
    "\n",
    "    def forward(self, batch_data):\n",
    "        node_features = self.feature_extractor(batch_data.x, batch_data.edge_index)\n",
    "        graph_features = self.pool(node_features, batch_data.batch)\n",
    "        return self.fc(graph_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition (GCN with GAT, previously defined)\n",
    "model = GraphBotDetector(feature_dim=256, num_classes=2).to_device(device)  # Adjust the feature_dim as per the actual feature vector length\n",
    "\n",
    "# Training preparation (Loss function, optimizer, dataloaders, etc.)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder function for splitting data based on Twibot-22 'split.csv'\n",
    "def load_splits():\n",
    "    # Implement loading of 'split.csv' and return train, valid, test indices\n",
    "    pass\n",
    "\n",
    "train_idx, valid_idx, test_idx = load_splits()\n",
    "\n",
    "# Prepare the dataset\n",
    "X_train, y_train = scaled_features[train_idx], labels[train_idx]\n",
    "X_valid, y_valid = scaled_features[valid_idx], labels[valid_idx]\n",
    "X_test, y_test = scaled_features[test_idx], labels[test_idx]\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float).to(device)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float).to(device)\n",
    "# ... do the same for X_valid_tensor, y_valid_tensor, X_test_tensor, y_test_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GraphBotDetector(feature_dim=256, num_classes=2).to(device)  # Feature dimension needs verification\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Placeholder for actual DataLoader implementation\n",
    "data_loader = DataLoader([Data(x=node_features, edge_index=edge_index)], batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = criterion(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example training call\n",
    "for epoch in range(10):  # Number of epochs should be adjusted based on actual needs\n",
    "    loss = train(model, data_loader, optimizer, criterion)\n",
    "    print(f'Epoch {epoch+1}: Loss {loss}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
