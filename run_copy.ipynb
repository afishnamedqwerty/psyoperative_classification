{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device='cuda'\n",
    "device = torch.device(device if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dask.dataframe as dd\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, ReLU\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.nn import GATConv, global_mean_pool\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import networkx as nx\n",
    "from datetime import datetime\n",
    "from random import choice, choices, sample\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "device='cuda'\n",
    "device = torch.device(device if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Data Paths\n",
    "DATA_PATHS = {\n",
    "    'users': 'cache/TwiBot-22/user.json',\n",
    "    'tweets': [f'cache/TwiBot-22/tweet_{i}.json' for i in range(9)],\n",
    "    'lists': 'cache/TwiBot-22/list.json',\n",
    "    'hashtags': 'cache/TwiBot-22/hashtag.json',\n",
    "    'edges': 'cache/TwiBot-22/edge.csv',\n",
    "    'labels': 'cache/TwiBot-22/label.csv',\n",
    "    'splits': 'cache/TwiBot-22/split.csv'\n",
    "}\n",
    "\n",
    "# Load data using Dask for efficiency with large datasets\n",
    "def load_data_with_dask(file_path):\n",
    "    ddf = dd.read_csv(file_path)\n",
    "    return ddf.compute()\n",
    "\n",
    "# Efficient data loading in chunks\n",
    "def load_data_in_chunks(file_path, chunk_size=1000000):\n",
    "    return pd.read_csv(file_path, chunksize=chunk_size)\n",
    "\n",
    "# Clear unused memory\n",
    "def clear_memory():\n",
    "    gc.collect()\n",
    "\n",
    "# Define the model\n",
    "class GraphBotDetector(torch.nn.Module):\n",
    "    def __init__(self, feature_dim, num_classes):\n",
    "        super(GraphBotDetector, self).__init__()\n",
    "        self.feature_extractor = GATConv(feature_dim, 128, heads=3, concat=True)\n",
    "        self.pool = global_mean_pool\n",
    "        self.fc = torch.nn.Linear(128 * 3, num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = F.relu(self.feature_extractor(x, edge_index))\n",
    "        x = self.pool(x, batch)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Convert dense matrix to sparse tensor\n",
    "def convert_to_sparse_tensor(matrix):\n",
    "    sparse_matrix = sp.coo_matrix(matrix)\n",
    "    values = torch.FloatTensor(sparse_matrix.data)\n",
    "    indices = torch.LongTensor([sparse_matrix.row, sparse_matrix.col])\n",
    "    return torch.sparse.FloatTensor(indices, values, torch.Size(sparse_matrix.shape))\n",
    "\n",
    "# Assuming 'edges' DataFrame has 'source_type' and 'target_type' columns that specify the type of the nodes\n",
    "def construct_heterogeneous_graph(edges):\n",
    "    H = nx.DiGraph()\n",
    "    for _, edge in edges.iterrows():\n",
    "        H.add_node(edge['source_id'], type=edge['source_type'])\n",
    "        H.add_node(edge['target_id'], type=edge['target_type'])\n",
    "        H.add_edge(edge['source_id'], edge['target_id'], type=edge['relationship_type'])\n",
    "    return H\n",
    "\n",
    "def heterogeneous_subgraph_sampling(graph, root_node, walk_length=30, num_walks=20):\n",
    "    subgraphs = []\n",
    "    for _ in range(num_walks):\n",
    "        current_node = root_node\n",
    "        subgraph_nodes = {current_node}\n",
    "        subgraph_edges = []\n",
    "\n",
    "        for _ in range(walk_length):\n",
    "            neighbors = [(neighbor, graph.edges[current_node, neighbor]['type']) for neighbor in graph.neighbors(current_node)]\n",
    "            if not neighbors:\n",
    "                break\n",
    "\n",
    "            # Weighted choice among neighbors to maintain diversity of edge types\n",
    "            next_node, _ = choices(neighbors, weights=[1 for _ in neighbors], k=1)[0]\n",
    "            subgraph_nodes.add(next_node)\n",
    "            subgraph_edges.append((current_node, next_node))\n",
    "            current_node = next_node\n",
    "\n",
    "        # Create the induced subgraph based on sampled nodes and add edges\n",
    "        sampled_subgraph = nx.DiGraph()\n",
    "        sampled_subgraph.add_nodes_from([(n, graph.nodes[n]) for n in subgraph_nodes])\n",
    "        sampled_subgraph.add_edges_from(subgraph_edges)\n",
    "        subgraphs.append(sampled_subgraph)\n",
    "    return subgraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'relationship_type'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Outrun\\anaconda3\\envs\\botCave\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'relationship_type'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 40\u001b[0m\n\u001b[0;32m     37\u001b[0m         G\u001b[38;5;241m.\u001b[39madd_edge(edge[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource_id\u001b[39m\u001b[38;5;124m'\u001b[39m], edge[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_id\u001b[39m\u001b[38;5;124m'\u001b[39m], attr\u001b[38;5;241m=\u001b[39medge[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelationship_type\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m G\n\u001b[1;32m---> 40\u001b[0m G \u001b[38;5;241m=\u001b[39m \u001b[43mload_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcache/TwiBot-22/edge.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 37\u001b[0m, in \u001b[0;36mload_graph\u001b[1;34m(edge_file)\u001b[0m\n\u001b[0;32m     35\u001b[0m G \u001b[38;5;241m=\u001b[39m nx\u001b[38;5;241m.\u001b[39mDiGraph()\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, edge \u001b[38;5;129;01min\u001b[39;00m edges\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m---> 37\u001b[0m     G\u001b[38;5;241m.\u001b[39madd_edge(edge[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource_id\u001b[39m\u001b[38;5;124m'\u001b[39m], edge[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_id\u001b[39m\u001b[38;5;124m'\u001b[39m], attr\u001b[38;5;241m=\u001b[39m\u001b[43medge\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelationship_type\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m G\n",
      "File \u001b[1;32mc:\\Users\\Outrun\\anaconda3\\envs\\botCave\\Lib\\site-packages\\pandas\\core\\series.py:1112\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m-> 1112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1114\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[0;32m   1115\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[0;32m   1116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[1;32mc:\\Users\\Outrun\\anaconda3\\envs\\botCave\\Lib\\site-packages\\pandas\\core\\series.py:1228\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[0;32m   1227\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m-> 1228\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[0;32m   1231\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[1;32mc:\\Users\\Outrun\\anaconda3\\envs\\botCave\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'relationship_type'"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Define feature extraction function\n",
    "def extract_user_features(users):\n",
    "    features = []\n",
    "    for user in users.itertuples():\n",
    "        # Calculate time delta in days\n",
    "        account_age_days = (datetime.now() - datetime.strptime(user.created_at, '%a %b %d %H:%M:%S +0000 %Y')).days\n",
    "        tweet_count = max(user.tweet_count, 1)  # Avoid division by zero\n",
    "\n",
    "        features.append([\n",
    "            user.tweet_count / account_age_days,  # tweet_frequency\n",
    "            user.retweet_count / tweet_count,     # retweet_ratio\n",
    "            user.mention_count / tweet_count,     # mention_ratio\n",
    "            user.hashtag_count / tweet_count,     # hashtag_ratio\n",
    "            user.url_count / tweet_count,         # url_ratio\n",
    "            user.sensitive_tweet_count / tweet_count,  # sensitive_content_ratio\n",
    "            account_age_days,                  # account_age_days\n",
    "            int(user.verified),                # is_verified\n",
    "            int(user.description != '') + int(user.location != '') + int(user.profile_image_url != ''),  # profile_completeness\n",
    "            user.followers_count / max(user.following_count, 1)  # follower_following_ratio\n",
    "        ])\n",
    "    return features\n",
    "\n",
    "# Load global edge data and construct the heterogeneous graph\n",
    "def load_graph(edge_file):\n",
    "    edges = pd.read_csv(edge_file)\n",
    "\n",
    "    # Initialize directed graph\n",
    "    G = nx.DiGraph()\n",
    "    for _, edge in edges.iterrows():\n",
    "        G.add_edge(edge['source_id'], edge['target_id'], relation=edge['relation'])\n",
    "    return G\n",
    "\n",
    "G = load_graph('cache/TwiBot-22/edge.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dask.dataframe as dd\n",
    "import networkx as nx\n",
    "import json\n",
    "\n",
    "class TwibotDataset(Dataset):\n",
    "    def __init__(self, graph, data_paths, transform=None):\n",
    "        \"\"\"\n",
    "        Initialize the dataset using global graph and data paths.\n",
    "        \"\"\"\n",
    "        self.graph = graph\n",
    "        self.data_paths = data_paths\n",
    "        self.transform = transform\n",
    "        self.nodes = self.load_nodes()\n",
    "\n",
    "    def load_nodes(self):\n",
    "        # Combine user data from multiple json files into a single DataFrame\n",
    "        users = pd.concat([pd.read_json(f) for f in self.data_paths['tweets']], ignore_index=True)\n",
    "        # Extract features\n",
    "        features_df = pd.DataFrame(extract_user_features(users))\n",
    "        return features_df\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the number of items in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.nodes)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get node features and ID\n",
    "        node_features = self.nodes.iloc[idx].values\n",
    "        node_id = self.nodes.index[idx]\n",
    "\n",
    "        # Create a dynamic subgraph centered at the node_id\n",
    "        subgraph = self.create_subgraph(node_id)\n",
    "\n",
    "        # Convert subgraph to PyTorch Geometric Data\n",
    "        data = self.subgraph_to_data(subgraph)\n",
    "\n",
    "        if self.transform:\n",
    "            data.x = self.transform(data.x)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def subgraph_to_data(self, subgraph):\n",
    "        \"\"\"\n",
    "        Convert a networkx subgraph to PyTorch Geometric Data.\n",
    "        \"\"\"\n",
    "        edge_index = torch.tensor(list(subgraph.edges())).t().contiguous()\n",
    "        x = torch.tensor([self.graph.nodes[n]['features'] for n in subgraph.nodes()], dtype=torch.float)\n",
    "        return Data(x=x, edge_index=edge_index)\n",
    "\n",
    "# Assuming preprocessing functions are defined, if any\n",
    "transform = torch.tensor  # Placeholder, define proper transformations based on model requirements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_paths = DATA_PATHS\n",
    "\n",
    "# Example usage of TwibotDataset and DataLoader\n",
    "dataset = TwibotDataset(G, DATA_PATHS)\n",
    "loader = DataLoader(dataset, batch_size=10, shuffle=True) #num_workers=4\n",
    "\n",
    "edges = load_data_with_dask(data_paths['edges'])\n",
    "graph = construct_heterogeneous_graph(edges)\n",
    "root_node = sample(list(graph.nodes), 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphBotDetector(torch.nn.Module):\n",
    "    def __init__(self, feature_dim, num_classes):\n",
    "        super(GraphBotDetector, self).__init__()\n",
    "        self.feature_extractor = GATConv(feature_dim, 128, heads=3, concat=True)\n",
    "        self.pool = global_mean_pool\n",
    "        self.fc = torch.nn.Linear(128 * 3, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = F.relu(self.feature_extractor(x, edge_index))\n",
    "        x = self.pool(x, torch.zeros(x.size(0), dtype=torch.long, device=x.device))\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Assume feature_dim correctly matches the output of extract_user_features\n",
    "model = GraphBotDetector(feature_dim=10, num_classes=2)\n",
    "optimizer = Adam(model.parameters(), lr=0.01)\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for data in loader:\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(data)\n",
    "    loss = criterion(output, data.y)  # Assuming target 'y' is correctly set up\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample subgraphs\n",
    "subgraphs = heterogeneous_subgraph_sampling(graph, root_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model and components\n",
    "model = GraphBotDetector(feature_dim=10, num_classes=2)\n",
    "model.to(device)\n",
    "optimizer = Adam(model.parameters(), lr=0.01)\n",
    "criterion = CrossEntropyLoss()\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "for epoch in range(10):  # Adjust based on dataset size and convergence observations\n",
    "    model.train()\n",
    "    for subgraph in subgraphs:\n",
    "        # Placeholder: convert subgraph to data instance\n",
    "        data = Data(x=subgraph.nodes['features'], edge_index=subgraph.edges['index'])\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data.x, data.edge_index, data.batch)\n",
    "        loss = criterion(output, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        writer.add_scalar('Loss/train', loss.item(), epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "model.eval()\n",
    "# Placeholder for evaluation logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()\n",
    "clear_memory()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "botCave",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
